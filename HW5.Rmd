---
title: 'HW5: Team [my team #/name here]'
author: '[My team member names here]'
date: " "
output:
  html_document: default
  html_notebook: default
  pdf_document: default
---


```{r setup, warning=FALSE, echo=FALSE}
knitr::opts_chunk$set(warning = FALSE)
suppressMessages(library(ggplot2))
suppressMessages(library(dplyr))
suppressMessages(library(GGally))
library(BAS)
library(knitr)
# post on piazza for additional packages if there are wercker build errors due to missing packages

```


We have seen that as the number of features in a model increases, the training error will necessarily decrease, but the test error may not.  
For this assignment we will explore this using simulation of data to compare methods for estimation and model selection.

Some "guideposts" for when to finish parts are provided within the problem set.

1.  Generate a dataset with $p = 20$ features and $n=1000$ as follows:
First let's set our random seed in case we need to rerun parts later.

```{r jenny, echo=TRUE}
# set the random seed so that we can replicate results.
set.seed(8675309)
```

In order to simulate data, we need to specify the values of the  "true" parameters.  For this study we will use

```{r true}
# true parameters
sigma = 2.5
betatrue = c(4,2,0,0,0,-1,0,1.5, 0,0,0,1,0,.5,0,0,0,0,-1,1,4)
#          int|    X1                            | X2     |X3 

truemodel = betatrue != 0
```



Generate Data with correlated columns.
```{r data, cache=TRUE} 

#sample size
n = 1000

# generate some standard normals
  Z = matrix(rnorm(n*10, 0, 1), ncol=10, nrow=n)
  
#  Create X1 by taking linear cominations of Z to induce correlation among X1 components
  
  X1 = cbind(Z, 
             (Z[,1:5] %*% c(.3, .5, .7, .9, 1.1) %*% t(rep(1,5)) +
             matrix(rnorm(n*5, 0, 1), ncol=5, nrow=n))
             )
# generate X2 as a standard normal  
  X2 <- matrix(rnorm(n*4,0,1), ncol=4, nrow=n)
  
# Generate X3 as a linear combination of X2 and noise  
  X3 <- X2[,4]+rnorm(n,0,sd=0.1)
  
# combine them  
  X <- cbind(X1,X2,X3)

# Generate mu     
# X does not have a column of ones for the intercept so need to add the intercept  
# for true mu  
mu = betatrue[1] + X %*% betatrue[-1] 
  
# now generate Y  
Y = mu + rnorm(n,0,sigma)  
  
# make a dataframe and save it
df = data.frame(Y, X, mu)
```






2. Split your data set into a training set containing $100$ observations and a test set containing $900$ observations.  Before splitting reset the random seed based on your team number
```{r}
set.seed(0)   # replace 0 with team number before runing
```





3.  Using Ordinary Least squares based on fitting the full model for the training data,  compute the average RMSE for a) estimating $\beta_{true}$, b) estimating $\mu_{true} = X_{\text{test}} \beta_{true}$ and c) out of sample prediction of $Y_{test}$ for the test data.
Note for a vector of length $d$, RMSE is defined as
$$
RMSE(\hat{\theta}) = \sqrt{\sum_{i = 1}^{d} (\hat{\theta}_j - \theta_j)^2/d}
$$
Provide Confidence/prediction intervals for $\beta$, and $\mu$, and $Y$ in the test data and report what percent of the intervals contain the true values.  Do any estimates seem surprising?


3. Perform best subset selection on the training data, and plot the training set RMSE for fitting associated with the best model of each size.

4. Plot the test set RMSE for prediction associated with the best model of each size.   For which model size does the test set RMSE take on its minimum value?  Comment on your results.  If it takes on its minimum value for a model withonly an intercept or a model containing all of the predictors, adjust $\sigma$ used in generating the data until the test set RMSE is minimized for an intermediate point.

5.  How does the model at which the test set RMSE is minimized compare to the true model used to generate the data?  Comment on the coefficient values and confidence intervals obtained from using all of the data.  Do the intervals include the true values?

6.  Use AIC with stepwise or all possible subsets to select a model based on the training data and then use OLS to estimate the parameters under that model.  Using the estimates to compute the RMSE for a) estimating $\beta^{true}$, b) estimating $\mu_{true}$ in the test data, and c) predicting $Y_{test}$. For prediction, does this find the best model in terms of RMSE? Does AIC find the true model?  Comment on your findings.

7.  Take a look at the summaries from the estimates under the best AIC model fit to the training data. Create confidence intervals for the $\beta$'s and comment on whether they include zero or not or the true value.

8.   Use BIC with either stepwise or all possible subsets to select a model and then use OLS to estimate the parameters under that model.  Use the estimates to compute the RMSE for a) estimating $\beta^{true}$, b) $\mu_{true}$ for the test data, and c) predicting $Y_{test}$.   For prediction, does this find the best model in terms of RMSE? Does BIC find the true model?  Comment on your findings.

9.  Take a look at the summaries from the estimates under the best AIC model fit to the training data. Create confidence intervals for the $\beta$'s and comment on whether they include zero or not or the true value.

10. Provide a paragraph  summarizing your findings and any recommendations for model selection and inference for the tasks of prediction of future data, estimation of parameters or selecting the true model.









